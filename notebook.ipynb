{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import api_nyt as nyt\n",
    "import api_wikipedia as wiki\n",
    "import manage_articles as mng\n",
    "import statistics\n",
    "import datetime\n",
    "import pickle\n",
    "import timeseries\n",
    "import TextPreprocessing as txt\n",
    "from datetime import date\n",
    "import graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import matching\n",
    "import time\n",
    "import word\n",
    "import random\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nwords = []\\nwith open(\"/Users/markus/Downloads/wikinewsPickle/words.txt\", \"rb\") as fp:\\n    words = pickle.load(fp)\\n\\nwords_shuffled = []\\nwith open(\"/Users/markus/Downloads/wikinewsPickle/words_shuffled.txt\", \"rb\") as fp:\\n    words_shuffled = pickle.load(fp)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#Load all articles from 2019 and a shuffled copy\n",
    "articles = mng.load_articles(\"/Users/markus/Downloads/wikinewsPickle/nyt2019.json\")\n",
    "articles_shuffled = mng.shuffle_publicationdates(articles)\n",
    "\n",
    "#Get a dict of dicts for each calendar week with word frequencies from getWordCounts\n",
    "wordCounts = mng.getWordCounts(articles)\n",
    "wordCounts_shuffled = mng.getWordCounts(articles_shuffled)\n",
    "\n",
    "#List of distinct words are the same for shuffled and non-shuffled articles\n",
    "distinctWords = mng.getDistinctWords(wordCounts)\n",
    "\n",
    "#List of lists of tuples containing weekly word frequency\n",
    "countsPerWeek = []\n",
    "for w in distinctWords:\n",
    "    countsPerWeek.append((w,mng.getCountPerWeek(wordCounts,w)))\n",
    "\n",
    "countsPerWeek_shuffled = []\n",
    "for w in distinctWords:\n",
    "    countsPerWeek_shuffled.append((w,mng.getCountPerWeek(wordCounts_shuffled,w)))    \n",
    "\n",
    "#Create list of word objects for each keyword\n",
    "words = []\n",
    "for c in countsPerWeek:\n",
    "    words.append(word.Word(c[0],ts_articles=timeseries.Timeseries(c[1])))\n",
    "words = sorted(words, key=lambda x: sum(x.ts_articles.getCounts()), reverse=True)\n",
    "\n",
    "words_shuffled = []\n",
    "for c in countsPerWeek_shuffled:\n",
    "    words_shuffled.append(word.Word(c[0],ts_articles=timeseries.Timeseries(c[1])))\n",
    "words_shuffled = sorted(words_shuffled, key=lambda x: sum(x.ts_articles.getCounts()), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n",
      "404 Not Found\n"
     ]
    }
   ],
   "source": [
    "top_amount = 150\n",
    "\n",
    "#Get top_amount of keywords from words, same for shuffled and non-shuffled\n",
    "keywords = []\n",
    "for i in range(top_amount):\n",
    "    keywords.append(words[i].keyword)\n",
    "\n",
    "#Get matching with wikipedia site\n",
    "m = matching.groupmatch(keywords, articles)\n",
    "m_shuffled = matching.groupmatch(keywords,articles_shuffled)\n",
    "\n",
    "#For each returned entry from groupmatch, add wikipedia site and wiki timeseries to word object\n",
    "for key in m.keys():\n",
    "    #Following line from \n",
    "    #https://stackoverflow.com/questions/7125467/find-object-in-list-that-has-attribute-equal-to-some-value-that-meets-any-condi\n",
    "    word = next((x for x in words if x.keyword == key), None)\n",
    "    query = m[key]['query']\n",
    "    page = m[key]['link'][1]\n",
    "    wiki_counts = wiki.get_counts(page, word.ts_articles.getStartDate(), word.ts_articles.getEndDate(),\"en\")\n",
    "    if wiki_counts is not None:\n",
    "        #If wikipedia timeseries exists\n",
    "        word.coocKeywords = query\n",
    "        word.wikipediaSite = page\n",
    "        word.ts_wiki = timeseries.parseWikipediaCounts(wiki_counts)\n",
    "\n",
    "    #Same procedure for shuffled words\n",
    "    word_shuffled = next((x for x in words_shuffled if x.keyword == key), None)\n",
    "    query_shuffled = m[key]['query']\n",
    "    page_shuffled = m[key]['link'][1]\n",
    "    wiki_counts = wiki.get_counts(page, word_shuffled.ts_articles.getStartDate(), word_shuffled.ts_articles.getEndDate(),\"en\")\n",
    "    if wiki_counts is not None:\n",
    "        word_shuffled.coocKeywords = query\n",
    "        word_shuffled.wikipediaSite = page\n",
    "        word_shuffled.ts_wiki = timeseries.parseWikipediaCounts(wiki_counts)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020_Democratic_Party_presidential_debates\n2020_Democratic_Party_presidential_debates\n"
     ]
    }
   ],
   "source": [
    "#Get list of all words with matched wikipedia site\n",
    "words_analyze = [x for x in words if x.wikipediaSite != \"\"]\n",
    "words_analyze_shuffled = [x for x in words_shuffled if x.wikipediaSite != \"\"]\n",
    "\n",
    "corr = []\n",
    "corr_shuffled = []\n",
    "\n",
    "#Drop all timepoints which are not contained in both article and wikipedia timeseries\n",
    "for w in words_analyze:\n",
    "    ts_a, ts_w = timeseries.alignTimeseries(w.ts_articles,w.ts_wiki)\n",
    "    if len(ts_w.getCounts())<2 or len(ts_a.getCounts())<2:\n",
    "        words_analyze.remove(w)\n",
    "    else:\n",
    "        corr.append(statistics.getCorrelation(ts_a.getCounts(),ts_w.getCounts()))\n",
    "for w in words_analyze_shuffled:\n",
    "    ts_a, ts_w = timeseries.alignTimeseries(w.ts_articles,w.ts_wiki)\n",
    "    if len(ts_w.getCounts())<2 or len(ts_a.getCounts())<2:\n",
    "        words_analyze_shuffled.remove(w)\n",
    "    else:\n",
    "        corr_shuffled.append(statistics.getCorrelation(ts_a.getCounts(),ts_w.getCounts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "correlation\n0.2321428606274169\n0.28399624372371424\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "print(\"correlation\")\n",
    "print(numpy.mean(corr))\n",
    "print(numpy.mean(corr_shuffled))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}